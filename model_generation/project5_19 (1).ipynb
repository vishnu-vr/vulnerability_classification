{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ieFqcwj9222"
   },
   "source": [
    "THE VERSION OF TENSORFLOW USED IS 1.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tBMol1vFW8V8",
    "outputId": "cdc29ddc-76ad-4ad4-f3df-a17a73e01600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UeB8mKFjyuuK",
    "outputId": "927dc090-537d-4a94-81a6-4261b2062376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfOYfnhM_DM7"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4qLRm649-xw"
   },
   "source": [
    "THE DIRECTORY FOR PREPROCESS FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHU7_QNbL33z"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/My Drive/project/clean_cve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPIdeaUU9S6n"
   },
   "outputs": [],
   "source": [
    "pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjF_lE24_H2T"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disable = True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Htt_dI94w5Uf",
    "outputId": "8f356370-fa3b-461c-9b35-b83ee45782e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/project/clean_cve'"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BsAomJy-Nvm"
   },
   "source": [
    "THIS IS WHERE THE MAGIC HAPPENS\n",
    "\n",
    "FOR DEEP LEARNING MODELS\n",
    "-----------------------\n",
    "1.THE DATASET IS SPLIT INTO TRAIN AND TEST <BR>\n",
    "2.FITTED THE KERAS TOKENIZER WITH ONLY TRAIN DATA<BR>\n",
    "3.APPLIED TEXT_TO_SEQUENCE OF TRAIN AND TEST <BR>\n",
    "4.FITTED THE MODEL WITH TRAIN DATA<BR>\n",
    "5.TESTED THE MODEL WITH TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "hlMuRggWMA7X",
    "outputId": "c3c83616-a7b7-444b-a80a-adf64fde93d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching combined stemmed words\n",
      "\n",
      "creating embedding_index\n",
      "\n",
      "vetorizing the text descriptions\n",
      "\n",
      "creating embedding layer for lstm\n",
      "\n",
      "model definition\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "--- 0.32239240407943726 minutes ---\n",
      "\n",
      "============================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential()\\nembedding_layer = Embedding(num_words,EMBEDDING_DIM,\\n                                embeddings_initializer=Constant(embedding_matrix),\\n                                input_length=max_length,\\n                                trainable=False,\\n                                name=\"embedding_layer\")\\n    \\nmodel.add(embedding_layer)\\n#model.add(Bidirectional(LSTM(units=100)))\\n#model.add(SeqWeightedAttention())\\nmodel.add(SeqSelfAttention())\\nmodel.add(Flatten())\\n\\n\\nmodel.add(Dense(100, activation=\\'relu\\'))\\nmodel.add(Dense(80, activation=\\'relu\\'))\\nmodel.add(Dense(40, activation=\\'relu\\'))\\nmodel.add(Dense(13,activation=\"softmax\"))\\nmodel.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'RMSprop\\', metrics=[\\'accuracy\\'])\\n\\nprint(\"\\nMODEL_SUMMARY\")\\nprint(model.summary())\\nprint(\"\\n\")\\n\\n\\n\\n#training the model\\nprint(\"model training\\n\")\\nhistory = model.fit(padded_X_train, y_train, batch_size=100, epochs=15, shuffle=True)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb  3 10:39:38 2020\n",
    "\n",
    "@author: vishnu\n",
    "\"\"\"\n",
    "#import preprocess.project5 as pre\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Flatten, CuDNNLSTM, Bidirectional, Dropout\n",
    "from keras import layers\n",
    "#from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "#from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#fetching preprocessed data upto stemming of all the description\n",
    "print(\"fetching combined stemmed words\\n\")\n",
    "#pre_text = p.combinedStemmedWords()\n",
    "path='shuffled_clean/shuffled_combined_description_after_stemming.json'\n",
    "pre_text = json.load(open(path))\n",
    "\n",
    "#finding maxlen parameter for padding\n",
    "max_length = max([len(s) for s in pre_text])\n",
    "\n",
    "#creating an embedding dictionary\n",
    "print(\"creating embedding_index\\n\")\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(\"all_in_one_20/cbow_model.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "#######NO NEED TO SHUFFLE AGAIN SINCE ITS ALREADY SHUFFLED######\n",
    "'''\n",
    "#shuffling features and labels (ie shuffling both in same order)\n",
    "indices = np.arange(padded_pre_text.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "padded_pre_text = padded_pre_text[indices]\n",
    "\n",
    "#importing labels\n",
    "labels = pd.read_csv(r\"labels.csv\")\n",
    "#making the label as of type array\n",
    "labels = labels[\"data\"].values\n",
    "#shuffling labels in the same order as features\n",
    "labels = labels[indices]\n",
    "'''\n",
    "#######NO NEED TO SHUFFLE AGAIN SINCE ITS ALREADY SHUFFLED######\n",
    "\n",
    "#importing labels\n",
    "labels = json.load(open(\"shuffled_clean/shuffled_labels.json\"))\n",
    "\n",
    "#one-hot encoding labels\n",
    "one_hot_labels = pd.get_dummies(labels)\n",
    "#converting to numpy array\n",
    "one_hot_labels = np.array(one_hot_labels)\n",
    "\n",
    "#splitting train and test into 80 and 20 respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_text, one_hot_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "#vetorize the text samples\n",
    "print(\"vetorizing the text descriptions\\n\")\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "#padding sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "padded_X_train = pad_sequences(X_train_seq, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "padded_X_test = pad_sequences(X_test_seq, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "#creating embedding layer for lstm\n",
    "print(\"creating embedding layer for lstm\\n\")\n",
    "num_words = len(word_index) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "model_count=0\n",
    "#defining model\n",
    "print(\"model definition\\n\")\n",
    "def create_model(optimizer,dense,num_filters,kernel_size):\n",
    "\n",
    "    #count for the number of models build\n",
    "    global model_count\n",
    "    model_count+=1\n",
    "\n",
    "    #making these global, otherwise wont be able to access inside the method\n",
    "    global embedding_matrix\n",
    "    global max_length\n",
    "    global EMBEDDING_DIM\n",
    "    global num_words\n",
    "    \n",
    "    print(\"\\nCONFIGS USED \"+\"FOR MODEL NO: \",model_count)\n",
    "    print(\"===============\")\n",
    "    print(\"optimizer - \",optimizer)\n",
    "    print(\"dense - \",dense)\n",
    "    #print(\"dropout - \",dropout)\n",
    "    '''\n",
    "    print(\"dropout - \",dropout)\n",
    "    print(\"learning_rate - \",learning_rate)\n",
    "    \n",
    "    \n",
    "    if optimizer == 'RMSprop':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                input_length=max_length,\n",
    "                                trainable=False,\n",
    "                                name=\"c_c_p\")\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    #model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    #model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    #model.add(layers.MaxPooling1D())\n",
    "     \n",
    "    #model.add(SeqWeightedAttention())\n",
    "\n",
    "    model.add(CuDNNLSTM(units=100, return_sequences=True))    \n",
    "\n",
    "    #model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    if dense == 3:\n",
    "      model.add(Dense(100, activation='relu'))\n",
    "      model.add(Dense(80, activation='relu'))\n",
    "      model.add(Dense(40, activation='relu'))\n",
    "\n",
    "    if dense == 2:\n",
    "      model.add(Dense(100, activation='relu'))\n",
    "      model.add(Dense(80, activation='relu'))\n",
    "\n",
    "    if dense == 1:\n",
    "      model.add(Dense(100, activation='relu'))\n",
    "\n",
    "    model.add(Dense(13, activation='softmax'))\n",
    "    \n",
    "    #compiling model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "params = dict(batch_size = [100,200,300,400,500,600,700,800,900,1000], \n",
    "                  epochs = [5,10,15,20],\n",
    "                  optimizer = ['RMSprop','Adam'],\n",
    "                  dense = [1,2],\n",
    "                  num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  #conv_layers = [2,3],\n",
    "                  #learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "                  #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "                  #dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "              )\n",
    "\n",
    "\n",
    "#grid = GridSearchCV(estimator=model, param_grid = params, cv=3)\n",
    "random = RandomizedSearchCV(estimator = model, param_distributions = params, cv = 5, return_train_score = True, verbose=1)\n",
    "#n_jobs=-1\n",
    "\n",
    "#random_result = random.fit(padded_X_train, y_train, validation_split = 0.2)\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "total_time = (stop_time - start_time)/60\n",
    "\n",
    "print(\"\\n============================================\\n\")\n",
    "print(\"--- %s minutes ---\" % total_time)\n",
    "print(\"\\n============================================\\n\")\n",
    "\n",
    "'''\n",
    "Emb+Conv+Conv+Pool+FC + lstm dirty\n",
    "\n",
    "#train-test spliting\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_pre_text, one_hot_labels, test_size=0.2)\n",
    "\n",
    "\n",
    "#training the model\n",
    "print(\"model training\\n\")\n",
    "model.fit(X_train, y_train, batch_size=256, epochs=15, validation_data=(X_test, y_test), shuffle=True)\n",
    "'''\n",
    "\n",
    "\n",
    "#############################################\n",
    "'''\n",
    "#model.add(CuDNNLSTM(units=100, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(units=100)))\n",
    "#model.add(SeqWeightedAttention())\n",
    "#model.add(SeqSelfAttention())\n",
    "#model.add(Flatten())\n",
    "\n",
    "'''\n",
    "#############################################\n",
    "'''\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                input_length=max_length,\n",
    "                                trainable=False,\n",
    "                                name=\"embedding_layer\")\n",
    "    \n",
    "model.add(embedding_layer)\n",
    "#model.add(Bidirectional(LSTM(units=100)))\n",
    "#model.add(SeqWeightedAttention())\n",
    "model.add(SeqSelfAttention())\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(13,activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nMODEL_SUMMARY\")\n",
    "print(model.summary())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#training the model\n",
    "print(\"model training\\n\")\n",
    "history = model.fit(padded_X_train, y_train, batch_size=100, epochs=15, shuffle=True)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1X1OI0vOnzq9"
   },
   "source": [
    "**GENERTAING PREDICITONS - DEEP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZiUUEUIn2Cd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "\n",
    "# GENERATE PREDICTIONS FOR FAILURE ANALYSIS\n",
    "def gen_predictions(folder,padded_X_test):\n",
    "  print(folder+'\\n')\n",
    "  model_name = folder+'/model.json'\n",
    "  weight = folder+'/weight.h5'\n",
    "  clf = load(model_name,weight)\n",
    "  pred = clf.predict_classes(padded_X_test,verbose=1)\n",
    "  pred = pred.tolist()\n",
    "  filename = folder.split('/')[-1]\n",
    "  print(\"\\n saving as json\")\n",
    "  saveAsJson(pred,filename)\n",
    "  # print(model_name)\n",
    "\n",
    "# CODE TO LOAD NECESSARY THE MODELS\n",
    "def load(model_name,weight):\n",
    "  print(\"loading...\\n\")\n",
    "  json_file = open(model_name, 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  CUSTOM_MODEL = model_from_json(loaded_model_json,{'SeqSelfAttention': SeqSelfAttention})\n",
    "  CUSTOM_MODEL.load_weights(weight, by_name=False)\n",
    "  print(\"loaded\")\n",
    "  return CUSTOM_MODEL\n",
    "\n",
    "# SAVE THE PREDICTIONS AS A JSON FILE\n",
    "def saveAsJson(data,filename):\n",
    "    filename += \".json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)  \n",
    "\n",
    "# path for saving the predictions\n",
    "os.chdir('/content/drive/My Drive/project/new_3/predictions/clean/deep')\n",
    "\n",
    "gen_predictions('/content/drive/My Drive/project/isham/new dirty models/BILSTM+seq clean - before split',padded_X_test)\n",
    "\n",
    "# path for the cnn models to be loaded from\n",
    "# path = '/content/drive/My Drive/project/new_3/saved_models/clean/cnn'\n",
    "# folder_list = os.listdir(path)\n",
    "# for folder in folder_list:\n",
    "#   # print(path+'/'+folder)\n",
    "#   folder = path+'/'+folder\n",
    "#   gen_predictions(folder,padded_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uupl0MJNvk4X"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ukRFJpxtHlc"
   },
   "outputs": [],
   "source": [
    "padded_X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6McFp7GN_APp"
   },
   "source": [
    "ARBITRARY CODE FOR TESTING INDIVIDUAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRwfJCjj1QU0"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "#from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Bidirectional(CuDNNLSTM(units=100, return_sequences=False)))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(80, activation='relu'))\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "#compiling model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"RMSprop\", metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nMODEL_SUMMARY\")\n",
    "print(model.summary())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#training the model\n",
    "print(\"model training\\n\")\n",
    "history = model.fit(padded_X_train, y_train, batch_size=200, epochs=5, shuffle=True, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZG4ZLGMD7OOT"
   },
   "source": [
    "**CV_RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09ipsjsuuGHs"
   },
   "outputs": [],
   "source": [
    "random_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bisgqT1jener"
   },
   "source": [
    "**BEST_PARAMS / BEST_ESTIMATOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KdBJ0mxCU5US",
    "outputId": "f52f013c-78b5-44e5-fa3b-320118557686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'RMSprop', 'num_filters': 32, 'kernel_size': 7, 'epochs': 5, 'dense': 2, 'batch_size': 300}\n"
     ]
    }
   ],
   "source": [
    "print(random_result.best_params_)\n",
    "random_model = random_result.best_estimator_.model\n",
    "random_history = random_result.best_estimator_.model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6K2aZ29mx9I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlOYb2zE_HwF"
   },
   "source": [
    "FOR SHALLOW LEARNING MODELS\n",
    "--------------------------\n",
    "1.THE DATASET IS SPLIT INTO TRAIN AND TEST<BR>\n",
    "2.DATASET IS CONVERTED TO SENTENCES AND LABELS ARE INTEGER ENCODED<BR>\n",
    "3.SINCE WE ARE USING PIPELINE CLASS WITH SHALLOW CLASSIFIERS, ONLY AT THE TIME OF FITTING THE MODEL TRAIN DATA IS PASSED TO THE PIPELINE. THIS ENSURES THAT ONLY TRAIN DATA IS FIT_TRANSFORMED ON COUNT_VECT AND TFIDF_TRANSFORMER.\n",
    "4.WHILE TESTING THE TEST DATA IS SIMPLY TRANSFORMED AND NOT FIT_TRANSFORMED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hVhfz4htYb3"
   },
   "source": [
    "**NAIVE BAYES, SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "mUwQoCgmtUYm",
    "outputId": "0845c767-a25a-46b5-ed5d-a755542e8e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching dataset...\n",
      "\n",
      "integer encoding labels...\n",
      "\n",
      "converting dataset...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import os \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive/project/clean_cve\")\n",
    "\n",
    "def integerEncode(labels):\n",
    "  #input : labels in text form\n",
    "  #output : integer encoded labels in a numpy array (shape = 1d) or list\n",
    "\n",
    "  ret = []\n",
    "\n",
    "  #dictionary containing labels as keys and int-encoding as values\n",
    "  encoding_dic = dict(zip(sorted(list(set(labels))),range(13)))\n",
    "\n",
    "  #dictionary containing int-encoding as keys and labels as values\n",
    "  inv_encoding_dic = dict(zip(range(13),sorted(list(set(labels)))))\n",
    "\n",
    "  #getting int-encoding for each labels and pushing to ret list\n",
    "  for labs in labels:\n",
    "    ret.append(encoding_dic[labs])\n",
    "\n",
    "  return ret, inv_encoding_dic\n",
    "\n",
    "\n",
    "def convertData(tokenized_des):\n",
    "  #input : get tokenized data as list of list\n",
    "  #output : combine it to form sentences(documents)\n",
    "       #and output is a list of sentences(documents)\n",
    "\n",
    "  ret = []\n",
    "  for data in tokenized_des:\n",
    "    ret.append(\" \".join(data))\n",
    "\n",
    "  return ret\n",
    "\n",
    "print(\"fetching dataset...\\n\")\n",
    "#importing descriptions\n",
    "dataset = json.load(open(\"shuffled_clean/shuffled_combined_description_after_stemming.json\"))\n",
    "\n",
    "#importing labels\n",
    "labels = json.load(open(\"shuffled_clean/shuffled_labels.json\"))\n",
    "\n",
    "print(\"integer encoding labels...\\n\")\n",
    "#integer encoding labeling\n",
    "encoded_labels ,inv_encoding_dic = integerEncode(labels)\n",
    "\n",
    "print(\"converting dataset...\\n\")\n",
    "#combining words to form docs\n",
    "dataset = convertData(dataset)\n",
    "\n",
    "#splitting train and test into 80 and 20 respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, encoded_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# ############NAIVE BAYES############\n",
    "\n",
    "# text_clf = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', MultinomialNB()),\n",
    "# ])\n",
    "\n",
    "# ############NAIVE BAYES############\n",
    "\n",
    "\n",
    "# '''\n",
    "# ############LINEAR SVM############\n",
    "# text_clf = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', SGDClassifier(loss='modified_huber', penalty='l2',\n",
    "#                           alpha=1e-3, random_state=42,\n",
    "#                           max_iter=5, tol=None, verbose=2)),\n",
    "# ])\n",
    "# ############LINEAR SVM############\n",
    "# '''\n",
    "\n",
    "\n",
    "# print(\"training started...\\n\")\n",
    "\n",
    "# parameters = {\n",
    "#   'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#   'tfidf__use_idf': (True,False),\n",
    "#   'clf__alpha': (1e-2, 1e-3),\n",
    "#   }\n",
    "\n",
    "\n",
    "# gs_clf = GridSearchCV(text_clf, parameters, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# gs_clf = gs_clf.fit(X_train, y_train)\n",
    "\n",
    "# stop_time = time.time()\n",
    "\n",
    "# total_time = (stop_time - start_time)\n",
    "\n",
    "# print(\"\\n============================================\\n\")\n",
    "# print(\"--- %s seconds ---\" % total_time)\n",
    "# print(\"\\n============================================\\n\")\n",
    "\n",
    "# print(\"training finished...\\n\")\n",
    "\n",
    "# print()\n",
    "# print(\"predicting...\\n\")\n",
    "# pred_start = time.time()\n",
    "# pred = gs_clf.predict(X_test)\n",
    "# pred_stop = time.time()\n",
    "\n",
    "# total_pred_time = (pred_stop - pred_start)/60\n",
    "\n",
    "# print(\"time for prediction...\\n\")\n",
    "# print(\"\\n============================================\\n\")\n",
    "# print(\"--- %s minutes ---\" % total_pred_time)\n",
    "# print(\"\\n============================================\\n\")\n",
    "\n",
    "# print(metrics.classification_report(y_test,pred,digits=4))\n",
    "#print(\"\\nbest params\\n\")\n",
    "#print(gs_clf.best_params_)\n",
    "print()\n",
    "\n",
    "#loc = \"/Users/vishnu/Desktop/project/new_saved_models/clean/naivebayes\"\n",
    "\n",
    "#print(\"saving model...\\n\")\n",
    "#dump(gs_clf, '/Users/vishnu/Desktop/project/new_saved_models/dirty/naivebayes/naivebayes.joblib')\n",
    "#print(\"saved\")\n",
    "#to save model\n",
    "#dump(clf, 'naivebayes.joblib')\n",
    "#to load model\n",
    "#clf = load('naivebayes.joblib') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDX-AVMgk5hr"
   },
   "source": [
    "**GENERATING PREDICTIONS - SHALLOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltEm4ZAAc0t2"
   },
   "outputs": [],
   "source": [
    "paths = ['/content/drive/My Drive/project/new_3/saved_models/clean/randomforest_tuned/randomforest_tuned.joblib',\n",
    "         '/content/drive/My Drive/project/new_saved_models/clean/decisiontree/decisiontree.joblib',\n",
    "         '/content/drive/My Drive/project/new_saved_models/clean/naivebayes/naivebayes.joblib',\n",
    "         '/content/drive/My Drive/project/new_saved_models/clean/svm/svm.joblib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "DH_0JMC_WS3H",
    "outputId": "8132a058-30f5-41c4-997f-42d21f1943d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/project/new_3/saved_models/clean/randomforest_tuned/randomforest_tuned.joblib\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/project/new_saved_models/clean/decisiontree/decisiontree.joblib\n",
      "\n",
      "/content/drive/My Drive/project/new_saved_models/clean/naivebayes/naivebayes.joblib\n",
      "\n",
      "/content/drive/My Drive/project/new_saved_models/clean/svm/svm.joblib\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def saveAsJson(data,filename):\n",
    "    filename += \".json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)  \n",
    "\n",
    "os.chdir('/content/drive/My Drive/project/new_3/predictions/clean/shallow')\n",
    "\n",
    "for model in paths:\n",
    "  print(model+'\\n')\n",
    "  clf = load(model)\n",
    "  pred = clf.predict(X_test)\n",
    "  pred = pred.tolist()\n",
    "  filename = model.split('/')[-1].split('.')[0]\n",
    "  saveAsJson(pred,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5WIriE7i0pn"
   },
   "outputs": [],
   "source": [
    "clf = load('/content/drive/My Drive/project/new_3/saved_models/dirty/stackingSHALLOW_SK_LEARN/stackingSHALLOW.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mJiHHXMlb7e"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Am1WzEwSCrCH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Generating report...\\n\")\n",
    "print(classification_report(encoded_labels,text_clf.predict(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1Pg1u_jT-ry"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/My Drive/project/saved_models/decisiontree_13_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skaAMLO_T2il"
   },
   "outputs": [],
   "source": [
    "dump(text_clf, 'randomforest.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQq4TLC9CKmj"
   },
   "source": [
    "**RANDOM FOREST/DECISION TREE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogVwiIwbbuXn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import numpy as np\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#os.chdir(\"/Users/vishnu/Desktop/project/dirty_cve/\")\n",
    "\n",
    "def integerEncode(labels):\n",
    "\t#input : labels in text form\n",
    "\t#output : integer encoded labels in a numpy array (shape = 1d) or list\n",
    "\n",
    "\tret = []\n",
    "\n",
    "\t#dictionary containing labels as keys and int-encoding as values\n",
    "\tencoding_dic = dict(zip(sorted(list(set(labels))),range(13)))\n",
    "\n",
    "\t#dictionary containing int-encoding as keys and labels as values\n",
    "\tinv_encoding_dic = dict(zip(range(13),sorted(list(set(labels)))))\n",
    "\n",
    "\t#getting int-encoding for each labels and pushing to ret list\n",
    "\tfor labs in labels:\n",
    "\t\tret.append(encoding_dic[labs])\n",
    "\n",
    "\treturn ret, inv_encoding_dic\n",
    "\n",
    "\n",
    "def convertData(tokenized_des):\n",
    "\t#input : get tokenized data as list of list\n",
    "\t#output : combine it to form sentences(documents)\n",
    "\t\t\t #and output is a list of sentences(documents)\n",
    "\n",
    "\tret = []\n",
    "\tfor data in tokenized_des:\n",
    "\t\tret.append(\" \".join(data))\n",
    "\n",
    "\treturn ret\n",
    "\n",
    "print(\"fetching dataset...\\n\")\n",
    "#importing descriptions\n",
    "dataset = json.load(open(\"shuffled_dirty/shuffled_combined_description_after_stemming_dirty.json\"))\n",
    "\n",
    "#importing labels\n",
    "labels = json.load(open(\"shuffled_dirty/shuffled_labels.json\"))\n",
    "\n",
    "print(\"integer encoding labels...\\n\")\n",
    "#integer encoding labeling\n",
    "encoded_labels ,inv_encoding_dic = integerEncode(labels)\n",
    "\n",
    "print(\"converting dataset...\\n\")\n",
    "#combining words to form docs\n",
    "dataset = convertData(dataset)\n",
    "\n",
    "#splitting train and test into 80 and 20 respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, encoded_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "############RANDOM FOREST############\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(verbose=2,n_jobs=-1)),\n",
    "])\n",
    "############RANDOM FOREST############\n",
    "\n",
    "'''\n",
    "############DecisionTreeClassifier############\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "############DecisionTreeClassifier############\n",
    "'''\n",
    "\n",
    "print(\"training started...\\n\")\n",
    "\n",
    "text_clf.fit(X_train,y_train)\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "total_time = (stop_time - start_time)\n",
    "\n",
    "print(\"\\n============================================\\n\")\n",
    "print(\"--- %s seconds ---\" % total_time)\n",
    "print(\"\\n============================================\\n\")\n",
    "print(\"training finished...\\n\")\n",
    "\n",
    "print()\n",
    "print(\"predicting...\\n\")\n",
    "pred_start = time.time()\n",
    "pred = text_clf.predict(X_test)\n",
    "pred_stop = time.time()\n",
    "\n",
    "total_pred_time = (pred_stop - pred_start)/60\n",
    "\n",
    "print(\"time for prediction...\\n\")\n",
    "print(\"\\n============================================\\n\")\n",
    "print(\"--- %s minutes ---\" % total_pred_time)\n",
    "print(\"\\n============================================\\n\")\n",
    "\n",
    "print(metrics.classification_report(y_test,pred,digits=4))\n",
    "#print(\"\\nbest params\\n\")\n",
    "#print(gs_clf.best_params_)\n",
    "print()\n",
    "\n",
    "#print(\"saving model...\\n\")\n",
    "#to save model\n",
    "#dump(clf, 'naivebayes.joblib')\n",
    "#to load model\n",
    "#clf = load('naivebayes.joblib') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WFL3y40ittpL",
    "outputId": "f8253bb8-6a7d-4fa9-afea-ad244fa08aed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(text_clf, 'randomforest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2AyFOMbnuAWr",
    "outputId": "181de4ab-358d-4085-90ac-243dca7baaa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomforest.joblib\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1VOaaSHtAya"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import numpy as np\n",
    "from joblib import dump, load\n",
    "\n",
    "clf = load('/content/drive/My Drive/project/saved_models/naivebayes_13_clean/naivebayes.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgOivuTfyBfc"
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGF5QZAZC3p1"
   },
   "source": [
    "**RUN THIS TO GET MORE GPU FIRE POWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_j1bdXelzv0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS8vxNs8UD_k"
   },
   "source": [
    "ANALYSIS SECTION\n",
    "-----------------\n",
    "CONTAINS CODE <BR>\n",
    "1.TO PREDICT SAMPLES<BR>\n",
    "2.CALCULATE ACC AND LOSS<BR>\n",
    "3.GENERATE CLASSIFICATION REPORT<br>\n",
    "4.PLOTTING VARIOUS GRAPHS<br>\n",
    "5.GENERATE MODEL FLOW CHART<br>\n",
    "6.SAVE MODELS<BR>\n",
    "7.LOAD SAVED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kw0Hf2QBNt-"
   },
   "source": [
    "**PREDICTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "o3C5c-JWMiv3",
    "outputId": "b633b89c-65e6-45b1-8d6b-7151f446e899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predicting...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "18434/18434 [==============================] - 9s 468us/step\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/project/clean_cve/shuffled_clean\")\n",
    "import json\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels=json.load(open('shuffled_labels.json'))\n",
    "labels=list(labels)\n",
    "one_hot_labels  = pd.get_dummies(labels)\n",
    "#converting to numpy array\n",
    "one_hot_labels = np.array(one_hot_labels)\n",
    "\n",
    "text=json.load(open('shuffled_combined_description_after_stemming.json'))\n",
    "\n",
    "pre_text = text\n",
    "\n",
    "#finding maxlen parameter for padding\n",
    "max_length = max([len(s) for s in pre_text])\n",
    "\n",
    "#pre_text = pre.preprocess(text,'')\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(pre_text)\n",
    "sequences = tokenizer_obj.texts_to_sequences(pre_text)\n",
    "padded_pre_text = pad_sequences(sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "#splitting train and test into 80 and 20 respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_pre_text, one_hot_labels, test_size=0.20, random_state=42)\n",
    "'''\n",
    "\n",
    "print(\"\\npredicting...\")\n",
    "\n",
    "result = CUSTOM_MODEL.predict_classes(padded_X_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUA5R-pl5evP"
   },
   "source": [
    "**ACCURACY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M4nrdpy8lusu",
    "outputId": "ac203b48-b328-4715-ea8a-809631903c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY :  0.9810675924921342\n"
     ]
    }
   ],
   "source": [
    "results=list(result)\n",
    "#converting to 1d\n",
    "y = y_test.argmax(-1)\n",
    "#s=sorted(list(set(labels)))\n",
    "#s=sorted(list(one_hot_labels))\n",
    "cur=0\n",
    "for i in range(len(results)):\n",
    "  #r=results[i]\n",
    "  #r=s[r]\n",
    "  if results[i] == y[i]:\n",
    "    cur+=1\n",
    "print(\"ACCURACY : \",cur/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDTYRyQ9N2fG"
   },
   "source": [
    "**VALIDATION (model.evaluate())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VmlNbbJ20ie"
   },
   "outputs": [],
   "source": [
    "#one_hot_labels  = pd.get_dummies(labels)             \n",
    "#model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "print(\"\\nvalidating...\\n\")\n",
    "re = CUSTOM_MODEL.evaluate(padded_X_test,y_test, verbose=1)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nK5ZKuyXbfl_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qubgrs4QePqX"
   },
   "source": [
    "**CONFUSION MATRIX - 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khSbwPobs7yE"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "norm_con_mat = con_mat / con_mat.astype(np.float).sum(axis=1)\n",
    "\n",
    "rows_and_cols = ['bypass',\n",
    " 'code_exec',\n",
    " 'cross_site_scripting',\n",
    " 'csrf',\n",
    " 'directory_traversal',\n",
    " 'dos',\n",
    " 'file_inclusion',\n",
    " 'gain_information',\n",
    " 'gain_privilege',\n",
    " 'http_response_splitting',\n",
    " 'memory_corruption',\n",
    " 'overflow',\n",
    " 'sql_injection']\n",
    "'''\n",
    "array = [[33,2,0,0,0,0,0,0,0,1,3], \n",
    "        [3,31,0,0,0,0,0,0,0,0,0], \n",
    "        [0,4,41,0,0,0,0,0,0,0,1], \n",
    "        [0,1,0,30,0,6,0,0,0,0,1], \n",
    "        [0,0,0,0,38,10,0,0,0,0,0], \n",
    "        [0,0,0,3,1,39,0,0,0,0,4], \n",
    "        [0,2,2,0,4,1,31,0,0,0,2],\n",
    "        [0,1,0,0,0,0,0,36,0,2,0], \n",
    "        [0,0,0,0,0,0,1,5,37,5,1], \n",
    "        [3,0,0,0,0,0,0,0,0,39,0], \n",
    "        [0,0,0,0,0,0,0,0,0,0,38]]\n",
    "'''\n",
    "\n",
    "df_cm = pd.DataFrame(norm_con_mat, index = [i for i in rows_and_cols],\n",
    "                  columns = [i for i in rows_and_cols])\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_ux-SmieY5N"
   },
   "source": [
    "**CONFUSION MATRIX - 2 (USE THIS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3Tl-xRsdJr7"
   },
   "outputs": [],
   "source": [
    "rows_and_cols = ['bypass',\n",
    " 'code_exec',\n",
    " 'cross_site_scripting',\n",
    " 'csrf',\n",
    " 'directory_traversal',\n",
    " 'dos',\n",
    " 'file_inclusion',\n",
    " 'gain_information',\n",
    " 'gain_privilege',\n",
    " 'http_response_splitting',\n",
    " 'memory_corruption',\n",
    " 'overflow',\n",
    " 'sql_injection']\n",
    "\n",
    "con=con_mat.tolist()\n",
    "\n",
    "converted=[]\n",
    "for row in con:\n",
    "  s = sum(row)\n",
    "  converted.append([round((ele/s),2) for ele in row])\n",
    "\n",
    "df_cm = pd.DataFrame(converted, index = [i for i in rows_and_cols],\n",
    "                  columns = [i for i in rows_and_cols])\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnsV4IOA_15T"
   },
   "source": [
    "**ROC CURVE CALCULATION FOR EACH CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1b2TDS-_eHU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "\n",
    "  #creating a set of all the unique classes using the actual class list\n",
    "  unique_class = set(actual_class)\n",
    "  roc_auc_dict = {}\n",
    "  for per_class in unique_class:\n",
    "    #creating a list of all the classes except the current class \n",
    "    other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "    #marking the current class as 1 and all other classes as 0\n",
    "    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "    #using the sklearn metrics method to calculate the roc_auc_score\n",
    "    roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "    roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "  return roc_auc_dict\n",
    "print(roc_auc_score_multiclass(labels, conv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "443O2errGyHT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSO5Gfi4nd0K"
   },
   "source": [
    "**CLASSIFICATION REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPRdjfGXMs2k"
   },
   "outputs": [],
   "source": [
    "DO NOT RUN THIS CELL\n",
    "\n",
    "'''\n",
    "\n",
    "##this is the list containing unique number of labels in sorted order\n",
    "s=sorted(list(set(labels))) \n",
    "\n",
    "conv_results=[]\n",
    "for res in list(result):\n",
    "  conv_results.append(s[res])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "z2Rn-Gd9M4ty",
    "outputId": "5a4ea315-9080-4b4c-c0d7-6d9f84504aa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9621    0.9703    0.9662       942\n",
      "           1     0.9945    0.9917    0.9931      6229\n",
      "           2     0.9908    0.9918    0.9913      2814\n",
      "           3     0.9771    0.9412    0.9588       408\n",
      "           4     0.9564    0.9824    0.9692       737\n",
      "           5     0.9977    0.9991    0.9984      3475\n",
      "           6     0.9091    0.5556    0.6897        18\n",
      "           7     0.9585    0.9428    0.9506      2010\n",
      "           8     0.9580    0.9730    0.9655       704\n",
      "           9     0.9677    0.9375    0.9524        32\n",
      "          10     0.5000    0.2000    0.2857        15\n",
      "          11     0.9053    0.9515    0.9278       804\n",
      "          12     0.9751    0.9553    0.9651       246\n",
      "\n",
      "    accuracy                         0.9811     18434\n",
      "   macro avg     0.9271    0.8763    0.8934     18434\n",
      "weighted avg     0.9810    0.9811    0.9809     18434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#ground_truth = np.argmax(one_hot_labels, axis=1)\n",
    "#print(ground_truth)\n",
    "#converting to id\n",
    "y = y_test.argmax(-1)\n",
    "con_mat = confusion_matrix(y, result)\n",
    "report = classification_report(y, result,digits=4)\n",
    "print(report)\n",
    "#print(f1_score(labels, conv_results,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaSjOQCT9MjC"
   },
   "source": [
    "**SAVE NUMPY ARRAY AS .npy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nogwd5fW7K6T"
   },
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "save(\"result_of_prediction_13_class_lstm.npy\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfanC0PE9Gqp"
   },
   "source": [
    "**LOAD NUMPY ARRAY FROM .npy FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCOaHVaq9Ccp"
   },
   "outputs": [],
   "source": [
    "# load numpy array from npy file\n",
    "from numpy import load\n",
    "# load array\n",
    "result = load('result_of_prediction_13_class_bilstm.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07P0Gw7e4nlB"
   },
   "outputs": [],
   "source": [
    "sorted(list(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTs_Xm6SyIR7"
   },
   "outputs": [],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w1k4qIFdUI-7"
   },
   "source": [
    "**SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOuBjptV1lWB"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/My Drive/project/new_3/saved_models/dirty/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxB827EUOx_3"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynkwh4V2R8X1"
   },
   "outputs": [],
   "source": [
    "mkdir cnn+lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IM882vMSAdF"
   },
   "outputs": [],
   "source": [
    "cd cnn+lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oQyGef2WSB4b",
    "outputId": "ba3d867d-8137-496e-c3b8-19acd14c8f5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJT1D1dVRmbI"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "#import os\n",
    "#os.chdir('/content/drive/My Drive/project/saved_models/dirty_models')\n",
    "print(\"saving...\\n\")\n",
    "#converting model to a JSON string\n",
    "model_json = random_model.to_json()\n",
    "#saving the JSON model\n",
    "with open(\"custom_model_13.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#saving the model weights\n",
    "random_model.save_weights(\"custom_model_13.h5\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpl5wNc3UQxg"
   },
   "source": [
    "**LOAD MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMVvOPudXrtU"
   },
   "outputs": [],
   "source": [
    "pip install keras_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OVbZd1uRi-GV",
    "outputId": "720027e1-a78a-486a-a447-fcb4975b2856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/project/new_3/saved_models/clean/cnn/Emb+Conv+Pool+Conv+FC\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/project/new_3/saved_models/clean/cnn/Emb+Conv+Pool+Conv+FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HgSHZYRTjyot",
    "outputId": "e7194de5-b3a0-4702-bcb9-700b4425ccaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_model_13.h5  custom_model_13.json\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "bt-OWInHuWad",
    "outputId": "8a94e7f9-e239-493c-c053-a3061eb41c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "#from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "#import json\n",
    "print(\"loading...\\n\")\n",
    "json_file = open('custom_model_13.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "CUSTOM_MODEL = model_from_json(loaded_model_json) #,{'SeqWeightedAttention': SeqWeightedAttention}\n",
    "CUSTOM_MODEL.load_weights(\"custom_model_13.h5\", by_name=False)\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "DPO6mlvZYpQn",
    "outputId": "d6084b2b-5da2-4914-85ab-26e61b8178e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.embeddings.Embedding at 0x7ff0c7900320>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7ff12ce36208>,\n",
       " <keras.layers.pooling.MaxPooling1D at 0x7ff0d02e7160>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7ff0d8e6e2b0>,\n",
       " <keras.layers.core.Flatten at 0x7ff0b4c7f080>,\n",
       " <keras.layers.core.Dense at 0x7ff0b4c85940>,\n",
       " <keras.layers.core.Dense at 0x7ff0b4c95f28>,\n",
       " <keras.layers.core.Dense at 0x7ff0b4c1af98>,\n",
       " <keras.layers.core.Dense at 0x7ff0b4c2f7f0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_MODEL.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrDMMWjUVY3"
   },
   "source": [
    "**PLOTTING ACC AND LOSS GRAPH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyEJpivMQasB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(history):\n",
    "\n",
    "  # Plot training & validation accuracy values\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot(random_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m94u13b1pGTq"
   },
   "source": [
    "**PLOT MODEL ARCHITECTURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlxJbFBDk6g5"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(random_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCZAA3N9pqLR"
   },
   "source": [
    "**CV_RESULTS AS PANDAS DAFATFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr-s_xD9po7S"
   },
   "outputs": [],
   "source": [
    "def showRandomResults(cv_results):\n",
    "  '''\n",
    "  displays the RandomizedSearchCV in a meaningful way with pandas\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  cv_results: result of random search cv\n",
    "\n",
    "  Output\n",
    "  ------\n",
    "  pandas dataframe\n",
    "  '''\n",
    "  from numpy import array\n",
    "  import numpy as np\n",
    "  from numpy.ma import masked_array\n",
    "  import pandas as pd\n",
    "\n",
    "  return pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "\n",
    "\n",
    "# cv_results={'mean_fit_time': array([22.43625231, 42.184761  , 26.3185616 , 38.86082087, 15.66873207,\n",
    "#         40.00621057, 22.48412013, 20.56461391, 52.08799605, 45.63473115]),\n",
    "#  'mean_score_time': array([0.24843063, 0.58034058, 0.63670082, 0.26256804, 0.29983168,\n",
    "#         0.26951008, 0.2412231 , 0.43775926, 0.35146742, 0.27748618]),\n",
    "#  'mean_test_score': array([0.97984611, 0.98334531, 0.982952  , 0.98257233, 0.97069129,\n",
    "#         0.9839692 , 0.96901016, 0.97372941, 0.98049716, 0.98376576]),\n",
    "#  'mean_train_score': array([0.98119907, 0.98543394, 0.98437606, 0.98493211, 0.97180372,\n",
    "#         0.98614935, 0.97092541, 0.97537401, 0.98468122, 0.98632905]),\n",
    "#  'param_batch_size': masked_array(data=[600, 100, 100, 600, 400, 600, 1000, 200, 300, 600],\n",
    "#               mask=[False, False, False, False, False, False, False, False,\n",
    "#                     False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'param_dense': masked_array(data=[1, 1, 3, 3, 3, 1, 2, 1, 2, 2],\n",
    "#               mask=[False, False, False, False, False, False, False, False,\n",
    "#                     False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'param_epochs': masked_array(data=[10, 10, 5, 20, 5, 20, 10, 5, 20, 20],\n",
    "#               mask=[False, False, False, False, False, False, False, False,\n",
    "#                     False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'param_optimizer': masked_array(data=['Adam', 'Adam', 'Adam', 'RMSprop', 'Adam', 'RMSprop',\n",
    "#                     'RMSprop', 'RMSprop', 'RMSprop', 'Adam'],\n",
    "#               mask=[False, False, False, False, False, False, False, False,\n",
    "#                     False, False],\n",
    "#         fill_value='?',\n",
    "#              dtype=object),\n",
    "#  'params': [{'batch_size': 600, 'dense': 1, 'epochs': 10, 'optimizer': 'Adam'},\n",
    "#   {'batch_size': 100, 'dense': 1, 'epochs': 10, 'optimizer': 'Adam'},\n",
    "#   {'batch_size': 100, 'dense': 3, 'epochs': 5, 'optimizer': 'Adam'},\n",
    "#   {'batch_size': 600, 'dense': 3, 'epochs': 20, 'optimizer': 'RMSprop'},\n",
    "#   {'batch_size': 400, 'dense': 3, 'epochs': 5, 'optimizer': 'Adam'},\n",
    "#   {'batch_size': 600, 'dense': 1, 'epochs': 20, 'optimizer': 'RMSprop'},\n",
    "#   {'batch_size': 1000, 'dense': 2, 'epochs': 10, 'optimizer': 'RMSprop'},\n",
    "#   {'batch_size': 200, 'dense': 1, 'epochs': 5, 'optimizer': 'RMSprop'},\n",
    "#   {'batch_size': 300, 'dense': 2, 'epochs': 20, 'optimizer': 'RMSprop'},\n",
    "#   {'batch_size': 600, 'dense': 2, 'epochs': 20, 'optimizer': 'Adam'}],\n",
    "#  'rank_test_score': array([ 7,  3,  4,  5,  9,  1, 10,  8,  6,  2], dtype=np.int32),\n",
    "#  'split0_test_score': array([0.98257273, 0.98535293, 0.98453921, 0.98406458, 0.98094529,\n",
    "#         0.98447144, 0.92412019, 0.98304743, 0.97836846, 0.98453921]),\n",
    "#  'split0_train_score': array([0.98262298, 0.98518294, 0.98462349, 0.98653919, 0.9804191 ,\n",
    "#         0.98565763, 0.9296782 , 0.98274165, 0.98165667, 0.98574239]),\n",
    "#  'split1_test_score': array([0.98291177, 0.98358989, 0.9829796 , 0.97368956, 0.97090936,\n",
    "#         0.98325086, 0.97870755, 0.98114872, 0.98243713, 0.98216587]),\n",
    "#  'split1_train_score': array([0.98399621, 0.98596275, 0.98479301, 0.9757061 , 0.97297662,\n",
    "#         0.98597974, 0.98016477, 0.98258907, 0.98596275, 0.98542029]),\n",
    "#  'split2_test_score': array([0.98101306, 0.98155558, 0.98162335, 0.98467487, 0.97958905,\n",
    "#         0.98420018, 0.97857189, 0.96731538, 0.98447144, 0.98420018]),\n",
    "#  'split2_train_score': array([0.98292816, 0.98509818, 0.98413181, 0.98775983, 0.98163968,\n",
    "#         0.98635268, 0.98001218, 0.97123045, 0.98872614, 0.98743773]),\n",
    "#  'split3_test_score': array([0.97063613, 0.98379219, 0.98318189, 0.98569101, 0.95775127,\n",
    "#         0.9845382 , 0.9827072 , 0.98128307, 0.97409469, 0.98392785]),\n",
    "#  'split3_train_score': array([0.97273976, 0.98597997, 0.98433554, 0.98787868, 0.96005899,\n",
    "#         0.98696321, 0.98353875, 0.98253852, 0.97955483, 0.98696321]),\n",
    "#  'split4_test_score': array([0.98209685, 0.98243594, 0.98243594, 0.98474163, 0.96426147,\n",
    "#         0.98338532, 0.98094398, 0.95585245, 0.98311406, 0.98399568]),\n",
    "#  'split4_train_score': array([0.98370826, 0.98494583, 0.98399645, 0.98677677, 0.96392423,\n",
    "#         0.98579347, 0.98123318, 0.95777035, 0.98750573, 0.98608166]),\n",
    "#  'std_fit_time': array([6.94142373, 2.23977214, 0.4425561 , 0.23692041, 0.61158227,\n",
    "#         0.25615192, 0.21333401, 0.70367266, 0.30395417, 0.2842092 ]),\n",
    "#  'std_score_time': array([0.00343445, 0.02178298, 0.00826929, 0.00485683, 0.0053856 ,\n",
    "#         0.00530831, 0.00255856, 0.01990091, 0.00300382, 0.00176009]),\n",
    "#  'std_test_score': array([0.00464939, 0.00128997, 0.00095963, 0.00447176, 0.00886734,\n",
    "#         0.00054521, 0.02249711, 0.01057974, 0.00379288, 0.00082773]),\n",
    "#  'std_train_score': array([0.00425904, 0.00044537, 0.00029681, 0.00464291, 0.00862985,\n",
    "#         0.00046924, 0.02066211, 0.00984609, 0.00350455, 0.00075664])}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project5@19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
