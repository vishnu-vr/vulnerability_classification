#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 11 09:56:14 2020

@author: vishnu
"""


import pandas as pd   
from nltk.stem import PorterStemmer
#import string
import re
import gensim
from gensim.models import Word2Vec, Phrases
import nltk.data
import nltk
from nltk.corpus import stopwords
import json

nltk.download('stopwords')

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')

stop_words = set(stopwords.words('english'))

###functions###

##finding similar words
#model.wv.most_similar("denial")

#function to load a w2v model
def loadW2v(name):
    name = '/Users/vishnu/Desktop/project/dirty_cve/'+name+'.model'
    ret = Word2Vec.load(name)
    
    return ret

#function to save w2v model
def saveW2vModel(model,name):
    name = '/Users/vishnu/Desktop/project/dirty_cve/'+name+'.model'
    model.save(name)
    
#function to save w2v model as ASCII
def saveW2vASCII(model,name):
    name = '/Users/vishnu/Desktop/project/dirty_cve/'+name+'.txt'
    model.wv.save_word2vec_format(name, binary=False)

#function to generate bigrams
####################################        
def bigram2vec(unigrams):
    bigrams = Phrases(unigrams)
    #model = gensim.models.Word2Vec(bigrams[unigrams])
    ret = bigrams[unigrams]

    return ret
#####################################

#function to vectorise words
def w2v(nested_list_of_words,mod_num,bigram=False):
    #input : nested list of words, mod_num represents which model to use
    #        (ie if 1 then cbow else if 2 then skip gram) and bigram to generate ngrams with n = 2
    #output : word vector corresponding to each words
    
    # Create bigrams instead of unigrams
    if bigram:
        nested_list_of_words = bigram2vec(nested_list_of_words)

    # Create CBOW model 
    if mod_num == 1:
        cbow_model = gensim.models.Word2Vec(nested_list_of_words, min_count = 1, size = 100, window = 5, iter = 5)
        return cbow_model
    # Create Skip Gram model 
    elif mod_num == 2:
        skip_gram_model = gensim.models.Word2Vec(nested_list_of_words, min_count = 1, size = 100, window = 5, sg = 1, iter = 5)
        return skip_gram_model

    

##function to stem
def stem(list_of_words):
    #input : tokenized nested list of words
    #output : nested list of stemmed tokens
    
    ret=[]
    for words in list_of_words:
        x=[]
        for word in words:
            x.append(PorterStemmer().stem(word))
        ret.append(x)
        
    return ret

##function to remove stop words
def removeStopWords(nested_list):
    #input : list after word tokenization
    #output : list with stop words removed
    
    ret=[]
    
    for sent in nested_list:
        s=[]
        for word in sent:
            if word not in stop_words:
                s.append(word)
        ret.append(s)
    
    return ret

##function to tokenzie words
def word_tokenize(text):
    #input : each sentence as string
    #output : list of tokenized words
    
    #converting to lower case
    text = text.lower()
    
    words = text.split()
    
    #table for removing '.' (dots)
    dot_table = str.maketrans('', '', '.')
    
    # table for removing punctuations except '.' (dots)
    non_dot_table = str.maketrans('', '', '!"#$%&\'()*+,-/:;<=>?@[\\]^_`{|}~')

    ret=[]
    
    
    for i in range(len(words)):
        #removing all punctuation except '.' (dots)
        words[i] = words[i].translate(non_dot_table)
                
        #checking for version tokens
        #second condition is to not find string such as 'a3v4b4n6a3c2'
        #only considering strings with dots in them as version numbers
        if bool(re.match(r'[a-z]*(\d+\.)+\d', words[i])):
            #if found retain version numbers 
            words[i] = re.search(r'[a-z]*([\d.]+)\d', words[i]).group(0)
        #checking for '.' (dots) in words other than version 
        elif bool(re.match(r'.', words[i])):
            #if found removes all dots and append
            words[i] = words[i].translate(dot_table)  
        #append to ret only if its not empty    
        if words[i] != '' and not words[i].isdigit():
            ret.append(words[i])
    
    return ret

'''
##function to tokenize sentences
def sent_tokenize(all_in_one_string):
    #input : string containing all the desciptions
    #output : list of tokenized sentences
    
    ret = re.split("\.\s+",all_in_one_string)
    
    return ret
'''

##function to concatinate all the description in a class to a single string in lowercase
def makeOne(des):
    #input : list of descriptions
    #output : concatinated string consisting all the desciptions in lowercase in a single stirng
    
    ret=""
    for d in des:
        ret += d.strip()+" . "
    
    ret = ret.rstrip()
    
    return ret.lower()

#function to save a list of words as csv
def saveAsCsv(data,filename):
    filename += '.csv'
    df = pd.DataFrame(data, columns=["data"])
    df.to_csv(filename, index=False)
    
##function to saving a list of words as txt
def saveAsTxt(list_of_words,filename):
    '''
    df = pd.DataFrame(list_of_words, columns=["tokens"])
    df.to_csv(filename, index=False)
    '''
    filename+=".txt"
    with open(filename, 'w') as f:
        for item in list_of_words:
            f.write("%s\n" % item)

def saveAsJson(data,filename):
    filename += ".json"
    with open(filename, 'w') as f:
        json.dump(data, f)    
        
def GenLabels(count):
    #input : count dictionary containing the number of sentences corresponding to each class label
    #output : list containg labels
    
    labels=[]
    
    for key,val in count.items():
        for i in range(val):
            labels.append(key)
            
    return labels
    

##function for preprocessing

#delaring empty dictionary for storing length of sentence list in each model_name
count_dic={}

def preprocess(des,model_name):
    #input : csv file and modal_name which is NOT the name of any model but the name of the csv file :)
    #output : preprocessing upto stemming
    
    global count_dic
    
    print("****"+model_name+"****\n")
    
    
    
    '''
    print("Combining descriptions...\n")
    
    #result is a string in lowercase
    all_in_one_string = makeOne(des)
    
    print("Tokenizing sentences...\n")
    
    #result is a list of tokenized sentences
    list_of_sentences = sent_detector.tokenize(all_in_one_string)
    '''
    
    
    print("Converting to list...\n")
    
    #converting the description to a python list
    list_of_sentences = list(des)
    
    print("Tokenizing words...\n")
    
    #result is a nested list containing lists of tokenized words
    #tokenizing list of sentences into nested list of words corresponding to each sentence
    nested_list_of_words=[]
    #taking each sentence in the list of sentences, converting everything to lowercase, 
    #word tokenizing them and appending them into list_of_words
    for sent in list_of_sentences:
        nested_list_of_words.append(word_tokenize(sent))
    
    '''
    print("removing stop words...\n")
    #removing stop words
    nested_list_of_words = removeStopWords(nested_list_of_words)
    '''
    
    print("Stemming words...\n")
    
    #result is list containing list of stemmed words corresponding to each sentences
    nested_list_of_words = stem(nested_list_of_words)
    
    print("removing empty sentences(final cleaning)...\n")
    
    #removing empty lists from nested_list
    nested_list_of_words_after_cleaning=[]
    #getting each list of words representing sentences
    for sent in nested_list_of_words:
        #if its not empty then we append else ignore
        if len(sent) != 0:
            nested_list_of_words_after_cleaning.append(sent)
    
    #updating the len of each nested_list_of_words(ie sentences) to the dictionary
    count_dic[model_name] = len(nested_list_of_words_after_cleaning)
    
    return nested_list_of_words_after_cleaning


def createWord2Vec(nested_list_of_words,model_name):
    #input : list containing tokenized sentences
    #ouput : generating both cbow and skip gram Word2Vec model and saving them
    
    print("Generating cbow word2vec model...\n")
    
    #result a model trained with nested list of words
    #creating a cbow word2vec model if second parameter is 1 else if second parameter is 2 then skip gram
    cbow = w2v(nested_list_of_words,1)
    
    print("Generating skip gram word2vec model...\n")
    
    skip_gram = w2v(nested_list_of_words,2)
    
    print("saving cbow word2vec model...\n")
    
    saveW2vModel(cbow, model_name+"_cbow")
    
    print("saving cbow word2vec model as ASCII...\n")
    
    saveW2vASCII(cbow, model_name+"_cbow")
    
    print("saving skip gram word2vec model...\n")
    
    saveW2vModel(skip_gram, model_name+"_sg")
    
    print("saving skip gram word2vec model as ASCII...\n")
    
    saveW2vASCII(skip_gram, model_name+"_sg")

    print("Complete...\n")

#for debugging purposes
def moonji(a):
    ind=0
    l=0
    for i,line in enumerate(a):
        if len(line)>l:
            l=len(line)
            ind=i
    
    print("length is ",l,'\n')
    print("index is ",ind)


def combinedStemmedWords():
    #output : list of combined nested words after stemming

    #############reading csv##################
    # reading code_execusion 
    code_exec = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/code_execution.csv")['description']
    
    # reading dos 
    dos = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/dos.csv")['description']

    # reading overflow 
    overflow = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/overflow.csv")['description']

    # reading cross_site_scripting 
    cross_site_scripting = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/cross_site_scripting.csv")['description']

    # reading gain_information 
    gain_information = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/gain_information.csv")['description']

    # reading sql_injection 
    sql_injection = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/sql_injection.csv")['description']

    # reading bypass 
    bypass = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/bypass.csv")['description']

    # reading memory_corruption 
    memory_corruption = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/memory_corruption.csv")['description']

    # reading gain_privilege 
    gain_privilege = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/gain_privilege.csv")['description']

    # reading directory_traversal 
    directory_traversal = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/directory_traversal.csv")['description']

    # reading csrf 
    csrf = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/csrf.csv")['description']

    # reading file_inclusion 
    file_inclusion = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/file_inclusion.csv")['description']

    # reading http_response_splitting 
    http_response_splitting = pd.read_csv(r"/Users/vishnu/Desktop/project/dataset/http_response_splitting.csv")['description']
    #############reading csv##################
    

    #########preprocessing the descriptions############

    
    combined = []

    code_exec = preprocess(code_exec,"code_exec")
    moonji(code_exec)
    combined += code_exec
    
    dos = preprocess(dos,"dos")
    moonji(dos)
    combined += dos

    overflow = preprocess(overflow,"overflow")
    moonji(overflow)
    combined += overflow

    cross_site_scripting = preprocess(cross_site_scripting,"cross_site_scripting")
    moonji(cross_site_scripting)
    combined += cross_site_scripting

    gain_information = preprocess(gain_information,"gain_information")
    moonji(gain_information)
    combined += gain_information

    sql_injection = preprocess(sql_injection,"sql_injection")
    moonji(sql_injection)
    combined += sql_injection

    bypass = preprocess(bypass,"bypass")
    moonji(bypass)
    combined += bypass

    memory_corruption = preprocess(memory_corruption,"memory_corruption")
    moonji(memory_corruption)
    combined += memory_corruption

    gain_privilege = preprocess(gain_privilege,"gain_privilege")
    moonji(gain_privilege)
    combined += gain_privilege

    directory_traversal = preprocess(directory_traversal,"directory_traversal")
    moonji(directory_traversal)
    combined += directory_traversal

    csrf = preprocess(csrf,"csrf")
    moonji(csrf)
    combined += csrf

    file_inclusion = preprocess(file_inclusion,"file_inclusion")
    moonji(file_inclusion)
    combined += file_inclusion

    http_response_splitting = preprocess(http_response_splitting,"http_response_splitting")
    moonji(http_response_splitting)
    combined += http_response_splitting
    
    return combined

#########preprocessing the descriptions############

###driver code###
# print("PREPROCESSING STARTED...\n")
# al = combinedStemmedWords()
# print("saving combinedStemmedWords as json...\n")
# saveAsJson(al,'combined_description_after_stemming_dirty')
# print("word2vec generation...\n")
# createWord2Vec(al, "all_in_one_20")
# print("generating labels...\n")
# labels=GenLabels(count_dic)
# print("saving labels as json...\n")
# saveAsJson(labels,'labels')
# print("saving labels as csv...\n")
# saveAsCsv(labels,'labels')
