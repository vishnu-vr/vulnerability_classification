'''
GENERATING DATAFRAME WITH COLUMNS
1.INDICES OF MISCLASSIFIED SAMPLES
2.ACTUAL LABEL OF THAT SAMPLE
3.PREDICTED (MISCLASSIFIED) LABEL OF THAT SAMPLE

STORES ALL THIS AS CSV
'''


import os
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

dataset_clean = json.load(open("/Users/vishnu/Desktop/project/clean_cve/shuffled_clean/shuffled_combined_description_after_stemming.json"))
dataset_dirty = json.load(open("/Users/vishnu/Desktop/project/dirty_cve/shuffled_dirty/shuffled_combined_description_after_stemming_dirty.json"))

dirty_labels = json.load(open("/Users/vishnu/Desktop/project/dirty_cve/shuffled_dirty/shuffled_labels.json"))
# one_hot
dirty_labels = np.array(pd.get_dummies(dirty_labels)).argmax(-1)

clean_labels = json.load(open("/Users/vishnu/Desktop/project/clean_cve/shuffled_clean/shuffled_labels.json"))
# one_hot
clean_labels = np.array(pd.get_dummies(clean_labels)).argmax(-1)

#splitting train and test into 80 and 20 respectively
# clean
C_X_train, C_X_test, C_y_train, C_y_test = train_test_split(dataset_clean, clean_labels, test_size=0.20, random_state=42)
# dirty
D_X_train, D_X_test, D_y_train, D_y_test = train_test_split(dataset_dirty, dirty_labels, test_size=0.20, random_state=42)

# taking the test data only
dirty_actual_labels = D_y_test
clean_actual_labels = C_y_test

def find_misclassified(predictions,dirty=True):
	'''
	description
	-----------
	Finds details about the misclassified samples.
	Default value of dirty = True

	parameters
	----------
	input (predictions)  : predicted classes from the classifier
	output			 	 : a dictionary with keys :-
						   actual_class 	 : actual classes (list)
						   predcited_class   : predcited classes (list)
						   indices 		     : indices of the description (list)
						 		 		       (index in the shuffled files)
	'''

	global dirty_actual_labels
	global clean_actual_labels

	# creating the dictionary
	detail_dic = {}

	detail_dic["actual_classes"] = []
	detail_dic["predicted_classes"] = []
	detail_dic["indices"] = []
	# detail_dic["count"] = 0

	if dirty:
		for i in range(len(dirty_actual_labels)):
			# checking for misclassifications
			if dirty_actual_labels[i] != predictions[i]:
				# appending details to the dictionary
				# detail_dic["count"] += 1
				# actual class
				detail_dic["actual_classes"].append(dirty_actual_labels[i])
				# predicted class
				detail_dic["predicted_classes"].append(predictions[i])
				# index of the sample
				detail_dic["indices"].append(i)
	else:
		for i in range(len(clean_actual_labels)):
			# checking for misclassifications
			if clean_actual_labels[i] != predictions[i]:
				# appending details to the dictionary
				# detail_dic["count"] += 1
				# actual class
				detail_dic["actual_classes"].append(clean_actual_labels[i])
				# predicted class
				detail_dic["predicted_classes"].append(predictions[i])
				# index of the sample
				detail_dic["indices"].append(i)
				

	return detail_dic



def find_intersection(detail_dic_of_best,detail_dic_of_model):
	'''
	description
	-----------
	Finds intersection of misclassified samples between the best and given model.

	parameters
	----------
	input   : misclassifion details of best and model to be compared
	output	: a dictionary with keys :-
			  common_indices 	  : indices of samples which are misclassified by both (list)
			  actual_classes 	  : actual classes (list)
			  class_pred_by_model : class predicted by the model to be compared
			  class_pred_by_best  : class predicted by the best model 
	'''

	# the indices of misclassified samples predicted by the best model
	mis_class_indices_best = list(detail_dic_of_best["indices"])
	# actual classes of the best model
	actual_classes_best = list(detail_dic_of_best["actual_classes"])
	# classes predicted by the best model
	pred_class_best = list(detail_dic_of_best["predicted_classes"])
	# the indices of misclassified samples predicted by model to be compared
	mis_class_indices_model = list(detail_dic_of_model["indices"])
	# classes predicted by the model to be compared
	pred_class_model = list(detail_dic_of_model["predicted_classes"])

	intersection_details = {}

	intersection_details["common_indices"] = []
	intersection_details["actual_classes"] = []
	intersection_details["class_pred_by_model"] = []
	intersection_details["class_pred_by_best"] = []
	# intersection_details["count"] = 0

	# taking each misclassified index
	for i in range(len(mis_class_indices_best)):
		# finding the indices of samples common to both
		# here mis_class_indices_best[i] is the sample index in the shuffled order
		if mis_class_indices_best[i] in mis_class_indices_model:
			# finding the list index where the sample index is located in mis_class_indices_model
			# this is found out to find the predicted class name by the model to be compared
			ind_of_pred_class_model = mis_class_indices_model.index(mis_class_indices_best[i])

			# intersection_details["count"] += 1
			# saving the miclasified sample index 
			intersection_details["common_indices"].append(mis_class_indices_best[i])
			# saving the actual class label
			intersection_details["actual_classes"].append(actual_classes_best[i])
			# class predicted by the model to be compared
			intersection_details["class_pred_by_model"].append(pred_class_model[ind_of_pred_class_model])
			# class predicted by the best model
			intersection_details["class_pred_by_best"].append(pred_class_best[i])

	return intersection_details


def find_difference(detail_dic_of_best,detail_dic_of_model):
	'''
	description
	-----------
	Finds the samples misclassified by the model to be compared only
	and not the samples miclasified by the best model
	result = (model - best)

	parameters
	----------
	input   : misclassifion details of best and model to be compared
	output	: a dictionary with keys :-
			  indices 	  		  : indices of samples which are misclassified by the
			  					    model to be compared with only (list)
			  actual_classes 	  : actual classes (list)
			  predcited_classes   : class predicted by the model to be compared
	'''


	# the indices of misclassified samples predicted by the best model
	mis_class_indices_best = list(detail_dic_of_best["indices"])
	# actual classes of the model to be compared
	actual_classes_model = list(detail_dic_of_model["actual_classes"])
	# the indices of misclassified samples predicted by model to be compared
	mis_class_indices_model = list(detail_dic_of_model["indices"])
	# classes predicted by the model to be compared
	pred_class_model = list(detail_dic_of_model["predicted_classes"])

	difference_details = {}

	difference_details["indices"] = []
	difference_details["actual_classes"] = []
	difference_details["predicted_classes"] = []
	# difference_details["count"] = 0

	for i in range(len(mis_class_indices_model)):
		# the sample that is not present in the samples miclasified
		# by the best model
		if mis_class_indices_model[i] not in mis_class_indices_best:
			# difference_details["count"] += 1
			# index of the sample
			difference_details["indices"].append(mis_class_indices_model[i])
			# actual class
			difference_details["actual_classes"].append(actual_classes_model[i])
			# predicted class
			difference_details["predicted_classes"].append(pred_class_model[i])


	return difference_details

def saveAsCsv(data,filename):
    filename += '.csv'
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)

def readCsv(path):
	data = pd.read_csv(path)
	data = dict(data)

	return data


# driver code
curdir = os.getcwd()

# ###################################
# # creating "find_misclassified" dic
# ###################################
# # path for saving the data
# os.chdir('/Users/vishnu/Desktop/project/new_3/failure_analysis/clean/deep')
# # path of predictions
# path = "/Users/vishnu/Desktop/project/new_3/predictions/clean/deep"

# files = os.listdir(path)

# for filee in files:
# 	filename = filee.split('.')[0]
# 	print('\n'+filename)
# 	try:
# 		predictions = json.load(open(path+'/'+filee))
# 		# print("done\n")
# 		# print(path+filee)
# 		dic = find_misclassified(predictions,dirty=False)
# 		# saving as csv
# 		saveAsCsv(dic,filename)
# 		# printing acc
# 		c = len(dic['actual_classes'])
# 		mis = (c/len(predictions))*100
# 		print()
# 		print(100-mis)
# 	except:
# 		print("An exception occurred")
# ###################################
# # creating "find_misclassified" dic
# ###################################


#####################################################
# creating "find_intersection & find_difference" dic
#####################################################
# save path
intersection_path = '/Users/vishnu/Desktop/project/new_3/failure_analysis/dirty/shallow/intersection_with_best'
difference_path = '/Users/vishnu/Desktop/project/new_3/failure_analysis/dirty/shallow/difference_with_best'

# best model
best_path = '/Users/vishnu/Desktop/project/new_3/failure_analysis/dirty/shallow/stackingSHALLOW.csv'
best_model = readCsv(best_path)

# path for rest of the models to be compared
path = '/Users/vishnu/Desktop/project/new_3/failure_analysis/dirty/shallow'
files = os.listdir(path)

for filee in files:
	if path+'/'+filee == best_path:
		continue
	try:
		# reading csv and converting to dic
		cur_model = readCsv(path+'/'+filee)
		# finding the intersection
		inter_dic = find_intersection(best_model,cur_model)
		# finding difference
		diff_dic = find_difference(best_model,cur_model)
		# filename of csv
		filename = filee.split('.')[0]
		# saving inter_dic
		saveAsCsv(inter_dic,intersection_path+'/'+filename)
		# saving diff_dic
		saveAsCsv(diff_dic,difference_path+'/'+filename)
	except:
		print("something went wrong!")
#####################################################
# creating "find_intersection & find_difference" dic
#####################################################


# # testing
# best_path = '/Users/vishnu/Desktop/project/new_3/failure_analysis/clean/deep/stackingDNN_clean_best.csv'
# best_model = readCsv(best_path)
# c_model = '/Users/vishnu/Desktop/project/new_3/failure_analysis/clean/deep/bilstm.csv'
# c_model = readCsv(c_model)

# dic = find_intersection(best_model,c_model)


os.chdir(curdir)












