import matplotlib.pyplot as plt
# from sklearn.metrics import confusion_matrix, f1_score
import seaborn as sn
import pandas as pd
from sklearn.model_selection import train_test_split
import os
import json
from tensorflow.python.keras.preprocessing.text import Tokenizer
import heapq
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
# this has been imported and its done purposefully
import matplotlib.pyplot as plot

def plot_pie(ratio_dic):
	# print(ratio_dic)
	'''
	ratio_dic : dictionary containig percentages and class numbers
	'''
	# Data to plot
	labels = list(ratio_dic.keys())
	sizes = list(ratio_dic.values())
	# colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']

	# initializing explode values for the largest slice
	explode = [0.0 for i in range(len(sizes))] 

	# finding the largest slice
	larger=sizes[0]
	index=0
	for ind,size in enumerate(sizes):
		if size > larger:
			larger = size
			index = ind
	explode[index] = 0.1

	# Plot
	# , colors=colors, wedgeprops={'edgecolor' : "black",'linewidth' : 3,}
	plt.pie(sizes, explode=explode, labels=labels,
	autopct='%1.0f%%', shadow=True, startangle=140, frame=False)
	
	# plt.axis('equal')
	# plt.show()

	#draw circle
	# centre_circle = plt.Circle((0,0),0.70,fc='white')
	# fig = plt.gcf()
	# fig.gca().add_artist(centre_circle)
	# Equal aspect ratio ensures that pie is drawn as a circle
	plt.axis('equal')  
	plt.tight_layout()
	plt.show()	

def heat_map(con_mat,percentage=True):
	'''
	percentage : whether to convert to percentage (dafault = true)
	'''
	if percentage:
		# converting to percentages
		# diving the elements of each row with the sum
		# of elements of that row
		percent_con_mat = []
		for row in con_mat:
			s = sum(row)
			new_row = []
			for ele in row:
				# round upto 2 decimal places
				d = (ele/s)*100
				new_row.append(round(d,2))
			percent_con_mat.append(new_row)

		con_mat = percent_con_mat

	df_cm = pd.DataFrame(con_mat)

	plt.figure(figsize = (15,10))
	sn.heatmap(df_cm, annot=True)
	plt.show()

def confusion_and_pie_data(descriptions,labels,pred_path):
	'''
	descriptions : x
	labels 		 : y
	pred_path 	 : folder path of the predictions 
	'''

	#splitting train and test into 80 and 20 respectively
	X_train, X_test, y_train, y_test = train_test_split(descriptions, labels, test_size=0.20, random_state=42)

	# contains only misclassified data
	'''
	"misclassified" contains distribution of misclassified labels to classes.
	for example : the value corresponding to key 0 contains the amount of samples
	misclassified to class 0.

	"actual" contains distribution of actual labels to classes.
	for example : the value corresponding to key 0 contains the amount of samples
	actually beloning to class 0.

	Both are dictionaries made for plotting pie chart
	'''
	# misclassified
	misclassified_pie = {}
	# actual
	actual_pie = {}
	# initialzing
	for i in range(13):
		misclassified_pie[i] = 0
		actual_pie[i] = 0

	'''list containing predicted and actual for plotting confusion matrix'''
	# (only added to this list if the sample is misclassified)
	misclassified_con_mat=[]
	actual_con_mat=[]

	# json prediction files of models
	pred_files = os.listdir(pred_path)

	# fetching predictions of each model (json file)
	for pred_json in pred_files:
		print(pred_json)
		if pred_json.split('.')[-1] != 'json':
			# print("asd")
			continue
		# predicition of the models
		pred = json.load(open(pred_path+'/'+pred_json))
		# checking whether misclassifed or not
		for i in range(len(pred)):
			# if actual != predicted : its misclassified
			if pred[i] != y_test[i]:
				# misclassified info
				misclassified_pie[pred[i]] += 1
				# actual info
				actual_pie[y_test[i]] += 1

				# list of prediction (misclassified)
				misclassified_con_mat.append(pred[i])
				# list of actual
				actual_con_mat.append(y_test[i])


	# dividing with the total count to 
	# convert to percentages
	# misclassified
	total_count_miss = sum(list(misclassified_pie.values()))
	misclassified_pie_p={}

	# actual
	total_count_actual = sum(list(actual_pie.values()))
	actual_pie_p={}

	# changes done :-
	# only converting to percent. Removed the code for rounding.

	for i in range(13):
		misclassified_pie_p[i] = (misclassified_pie[i]/total_count_miss)*100
		actual_pie_p[i] = (actual_pie[i]/total_count_actual)*100

	'''first list contains two dictionaries for plotting pie charts'''
	'''second list contains two lists for plotting confusion matrix'''
	return [actual_pie_p, misclassified_pie_p], [actual_con_mat, misclassified_con_mat]


def get_misclassified_test_data(from_class,to_class,pred_path,descriptions,labels):
	'''
	from_class	 : the class number to which the test samples actually belong to
	to_class  	 : the class number to which the test samples got misclassified into
	pred_path 	 : folder path where the predictions of models are present
	descriptions : list of descriptions
	labels       : list of labels
	'''

	X_train, X_test, y_train, y_test = train_test_split(descriptions, labels, test_size=0.20, random_state=42)

	# json prediction files of models
	pred_files = os.listdir(pred_path)

	# storing index values of samples
	already_seen_indices = []

	test_data = []
	# fetching predictions of each model (json file)
	for pred_json in pred_files:
		# print(pred_json)
		# if not json then continue
		if pred_json.split('.')[-1] != 'json':
			# print("asd")
			continue
		# predicition of the models
		pred = json.load(open(pred_path+'/'+pred_json))
		# checking whether misclassifed or not
		for i in range(len(pred)):
			# if actual != predicted : its misclassified
			if pred[i] != y_test[i]:
				# if misclassified then find whether actual is "from_class" 
				if y_test[i] == from_class:
					# if sample belongs to "from_class"
					# then check whether it is predicted to "to_class"
					if pred[i] == to_class:
						if i not in already_seen_indices:
							already_seen_indices.append(i)
							# finding the exact test description
							test_data.append(X_test[i])
	return test_data

# print(len(get_misclassified_test_data(9,2,dirty_pred_path,dirty_des,dirty_labels)))

def get_test_data(class_number,descriptions,labels):
	'''
	class_number : the class from which the test data is to be fetched
	descriptions : list of descriptions
	labels       : list of labels
	'''

	X_train, X_test, y_train, y_test = train_test_split(descriptions, labels, test_size=0.20, random_state=42)

	test_data = []

	for i in range(len(y_test)):
		# if this is the class we want
		if y_test[i] == class_number:
			test_data.append(X_test[i])

	return test_data

# print(len(get_test_data(9,dirty_des,dirty_labels)))

def get_train_data(class_number,descriptions,labels):
	'''
	class_number : the class from which the train data is to be fetched
	descriptions : list of descriptions
	labels       : list of labels
	'''

	X_train, X_test, y_train, y_test = train_test_split(descriptions, labels, test_size=0.20, random_state=42)

	train_data = []

	for i in range(len(y_train)):
		# if this is the class we want
		if y_train[i] == class_number:
			train_data.append(X_train[i])

	return train_data

# print(len(get_train_data(9,dirty_des,dirty_labels)))

def get_ind_larg_two(row):
	# yes i know this can be shorten by sorting
	# but whats the fun in that ? ;)
	'''
	get the index of the largest two values
	'''
	# largest and second largest
	lar_val, s_lar_val, lar_ind, s_lar_ind = -1 ,-1, -1, -1
	for i in range(len(row)):
		if row[i] > lar_val:
			s_lar_val = lar_val
			lar_val = row[i]
			# indices
			s_lar_ind = lar_ind
			lar_ind = i
		elif row[i] > s_lar_val:
			s_lar_val = row[i]
			# indices
			s_lar_ind = i 
			

	return [lar_ind,s_lar_ind]

def summarizer(text,top_n_sent=None):
	'''
	link : https://stackabuse.com/text-summarization-with-nltk-in-python/
	this is a modified version

	description
	------------
	find summary of the text given

	text 	   : in a list of list manner 
	top_n_sent : length of the summary (default = '50' percent)
	'''
	if top_n_sent == None:
		top_n_sent = round(len(text)/2)

	# total_length = 0
	# for sent in text:
	# 	for word in sent:
	# 		# extra one is for spaces
	# 		total_length += (len(word) + 1)
	# 	# there wont a space after the last word
	# 	total_length -= 1

	# top_n_sent = round(top_n_sent*total_length)

	tokenizer_obj = Tokenizer()
	tokenizer_obj.fit_on_texts(text)

	# word frequency dictionary from keras tokenizer
	word_frequencies = tokenizer_obj.word_counts

	maximum_frequncy = max(word_frequencies.values())
	# normalizing with the max size
	for word in word_frequencies.keys():
	    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)


	# stopwords = nltk.corpus.stopwords.words('english')

	# # finding frequency of each word
	# word_frequencies = {}
	# for word in nltk.word_tokenize(formatted_article_text):
	#     if word not in stopwords:
	#         if word not in word_frequencies.keys():
	#             word_frequencies[word] = 1
	#         else:
	#             word_frequencies[word] += 1

	# calculating sentence score
	sentence_scores = {}
	for sent in text:
		# while "sent" is a list of words
		# "real_sent" is a string
		real_sent = ' '.join(sent)
		for word in sent:
			if word in word_frequencies.keys():
				if real_sent not in sentence_scores.keys():
					sentence_scores[real_sent] = word_frequencies[word]
				else:
					sentence_scores[real_sent] += word_frequencies[word]


	summary_sentences = heapq.nlargest(top_n_sent, sentence_scores, key=sentence_scores.get)

	'''list of string manner'''
	return summary_sentences

# link : https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50
def get_cosine_sim(*strs): 
    vectors = [t for t in get_vectors(*strs)]
    return cosine_similarity(vectors)
    
def get_vectors(*strs):
    text = [t for t in strs]
    vectorizer = CountVectorizer()
    vectorizer.fit(text)
    return vectorizer.transform(text).toarray()

def convertData(tokenized_des):
  #input : get tokenized data as list of list
  #output : combine it to form sentences(documents)
       #and output is a list of sentences(documents)

  ret = []
  for data in tokenized_des:
    ret.append(" ".join(data))

  return ret

def plot_hbar(list_1,list_2,labels):

	# link : https://pythontic.com/pandas/dataframe-plotting/bar%20chart 

	data = {"list_1":list_1,"list_2":list_2}

	index = [i for i in range(13)]

	# Dictionary loaded into a DataFrame       
	dataFrame = pd.DataFrame(data=data, index=index)

	# Draw a vertical bar chart
	ax = dataFrame.plot.barh(legend=False,rot=0, width=0.8)
	ax.set_xlabel("Similarity")
	ax.set_ylabel("Class")
	
	for i in range(len(ax.patches)):
		p=ax.patches[i]
		ax.annotate(str(labels[i]), (p.get_width(), p.get_y()-0.25), xytext=(5, 10), textcoords='offset points')
		# p.get_x() + p.get_width()
		
	plot.show(block=True)

def saveAsJson(data,filename):
    filename += ".json"
    with open(filename, 'w') as f:
        json.dump(data, f) 